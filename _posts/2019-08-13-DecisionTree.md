---
layout: post
title: "DecisionTree"
author: "JeonghyunGan"
categories: [Blog]
use_math: true
author_profile: true
css: ['typo.css']
---

## 1. Overview

![Tree](https://i.stack.imgur.com/iXc11.png)

Decision Tree는 결국 Y/N으로 답할 수 있는 질문들을 통한 의사결정 모델이다. 즉 Decision Tree 모델링이란 가장 빨리 정답에 도달할 수 있는 질문, 타겟에 대해 가장 확실한 정보를 얻을 수 있는 질문이 무엇인지를 학습하는 과정이다. 그렇다면 '확실한 정보를 제공하는 질문'의 기준은 무엇일까? 다양한 방법론들이 존재하지만 대표적으로 사용되는 분기 기준은 다음과 같다.

- 범주형 타겟: **엔트로피**, **지니**, 카이스퀘어
- 연속형 타겟: RSS(Residual Sum of Square, 잔차제곱합)

Decision Tree에는 크게 다음과 같은 두 가지 형태의 알고리즘이 존재하지만, 현재에는 CART가 가장 널리 쓰인다.

- ID3, C4.5
- CART

## 2. ID3

ID3는 **엔트로피** 를 활용한 알고리즘이다. ID3 알고리즘의 작동 과정은 다음과 같다.

1. 전체 노드를 포함하는 루트 노드를 생성한다
2. 만약 샘플들이 모두 같은 클래스라면, 노드는 잎이 되고 해당 클래스로 레이블을 부여한다.
3. 모두 같은 클래스가 아니라면 정보이득이 높은 속성을 선택한다
4. 해당 속성으로 가지를 뻗어서 하위 노드들을 생성한다
5. 각 노드들에 대하여 2단계로 돌아가 반복한다.

ID3, information gain이 지니는 한계점들

1. 연속형 타겟에 적용 불가능하다
2. 선택된 속성에 범주가 매우 많은 경우, 가지 수가 많아진다(Binary를 사용하지 않는 경우).
3. 속성에 포함된 값이 다양할수록 선택이 잘 됨

### 1) 정보함수

$Information(x) = log_2\frac1{p(x)}$  

![정보함수](/assets/article_images/infofunc.png)

정보함수는 정보의 가치를 나타내는 함수이다. 위 그래프에서 보다시피 **사건의 확률과 정보의 가치는 반비례** 한다. 희귀한 사건은 그 빈도가 낮고, 따라서 예측하기가 어려우며 가치가 높다. 반대로 확률이 큰 사건은 빈번하게 일어나고, 예측하기 쉬우며, 따라서 정보의 가치는 작아진다. 우리 데이터로 따지면 비행기가 지연되는 사건은 정보의 가치가 크고, 정상운항하는 사건은 정보의 가치가 작다고 말할 수 있다.

### 2) 엔트로피

$Entropy(D) = -\sum_{i=1}^m p_ilog_2(p_i)$

- $D$ = 데이터셋
- $p_i$ = 라벨 i의 확률

![엔트로피](/assets/article_images/entropy.png)

>Higher entropy > Higher Uncertainty (예측 불가능)  
Lower entropy > Lower Uncertainty (예측 가능)

엔트로피는 정보의 불확실성을 표현하는 수치로, 최소값은 0이며 최대값은 타겟이 갖는 범주의 수에 따라서 달라진다. 예를 들어서 타겟이 가질 수 있는 값이 $2^k$개라면 각 범주에 대한 확률이 모두 같을 때, 즉 $\frac{1}{2^k}$일 때 엔트로피가 최대이며 이 값은 k가 된다.

$$Entropy = -2^k\frac{1}{2^k}log_2\frac{1}{2^k}=k$$

![엔트로피2](/assets/article_images/entropy2.png))

즉 일반적으로 분류 라벨이 많아질수록, 엔트로피가 가질 수 있는 최대값은 증가한다. 하지만 분류 라벨이 많아질수록 반드시 엔트로피가 커진다고 할 수는 없다. 엔트로피는 결국 타겟의 각 라벨들이 갖는 (확률 X 정보함수)의 합으로 표시된다. 확률이 낮으면 정보함수의 값은 커진다. 확률이 커지면 정보함수의 값은 낮아진다. 즉 (확률 X 정보함수)의 합으로 표현되는 엔트로피의 구조에는 일정한 Trade-Off가 존재한다. 위 그래프를 보면 $p=e^-1$일 때 (확률 X 정보함수) 값이 최대가 된다는 사실을 알 수 있다. 따라서 분류 라벨이 많아질수록 개별 라벨의 확률이 감소하여 엔트로피가 증가하는 경향이 있을 수는 있지만, 반드시 그런 것은 아니다.

### 3) 정보획득량

정보획득량은 데이터셋의 총 엔트로피에서 특정한 속성을 통해 분류할 때의 엔트로피를 뺀 값이다. 즉 정보획득량이 커지려면(보다 많은 정보를 얻으려면), 속성을 통해 분류했을 때의 엔트로피가 작아져야만 한다. 1~v까지의 범주를 갖는 속성 A에 대한 엔트로피는 다음과 같이 표현된다.

$Entropy_A(D) = -\sum_{j=1}^v[\frac{D_j}{D}\sum_{i=1}^{m}p_ilogp_i \hspace{0.3cm}where\hspace{0.3cm}A=j]$

즉 특정 범주에서의 엔트로피에 데이터의 개수만큼 가중치를 주어 더한 것이 속성별 엔트로피다. 이제 정보획득량을 다음과 같이 표현할 수 있다.

$Gain(A) = Entropy(D)-Entropy_A(D)$
