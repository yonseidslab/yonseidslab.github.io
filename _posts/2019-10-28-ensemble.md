---
layout: post
title: "앙상블"
author: "JeonghyunGan"
categories: [Blog]
use_math: true
---

복수의 예측 모델을 결합하는 것을 앙상블이라고 한다. 앙상블의 일반적인 효과는 다음과 같다.

1. 단일 모델을 사용할 때 보다 분산이 감소한다. 즉 과적합을 방지한다.
2. 개별 모델의 성능이 좋지 않은 경우, 결합 모델의 성능이 더 향상된다.

모델 결합 방법은 크게 **aggragation(병렬적인 방법)**과 **boosting(순차적인 방법)** 으로 나뉜다.

## 1. Aggregation

### 1.1. Voting Classifier

Voting은 말 그대로 투표 방식의 앙상블을 뜻하고, 여러 분류기가 내놓은 결과를 투표로 취합하여 최종 결과를 선정한다. 즉 전혀 다른 모델 간의 결합도 가능하며, 분류 문제에만 적용할 수 있다.

Voting은 다시 hard voting, soft voting 으로 나뉜다. Hard voting은 결과만을 가지고 투표한다. 반면 soft voting은 각 모델이 내놓은 예측 확률을 평균내어 가장 확률이 높은 클래스를 선택한다. 따라서 예측에 대한 확률값을 내놓지 않는 분류기는 soft voting에 활용할 수 없다. 추가적으로, 원하는 모델에 가중치를 주어 가중 투표를 실시하는 것도 가능하다(scikit-learn에서는 `weights` 인자에 원하는 가중치 리스트를 전달하면 된다)

다수결 모델이 정말 개별 모델보다 더 나은 성능을 보이는지 실험을 통해 확인해보자. 만약 개별 모델의 정답률이 $p$인 모델을 $N$개 모아서 다수결 모델을 만들면, 앙상블 모델의 정답률은 다음과 같다. 개별 모델의 성능이 일정 수준 이상으로 올라가면, 단일 모델에 비해 앙상블 모델의 성능이 좋아진다는 것을 알 수 있다.

$$\sum^N_{k > \frac{N}{k}} {n \choose x} p^k(1-p)^{N-k} $$

### 1.2. Bagging

모델 결합에 사용되는 독립적인 모델의 수가 많을수록 앙상블 모델의 성능이 좋아지기는 하지만, 현실적으로 각기 다른 모델을 사용하는데는 한계가 있다. 따라서 보통은 **동일한 모델과 모수를 사용하는 대신 데이터를 랜덤하게 선택해서 voting을 적용** 한다. 트레이닝 데이터를 선택하는 방법에 따라서 배깅 모델을 다음과 같이 부르기도 한다.

Ensemble Mehods|Description|
---|---|
Pasting|중복을 허용하지 않고 샘플링
Bagging|중복을 허용하여 샘플링(bootstrap&aggragation)
Random Subspace|독립 변수 충 일부를 선택
Random Patches|데이터 샘플과 변수 모두 일부만 무작위 선택

### 1.3. Random Forest

랜덤포레스트의 특징은 다음과 같다.

- 의사결정나무를 기반으로 한 voting(hard, soft 모두 가능) 앙상블
- 개별 트리는 무작위로 n개의 변수를 선택한 후 그 중에서 가장 gain이 높을 변수를 선택해서 분기
- Extreamly Randomized Trees: 무작위 변수 하나를 가지고 분기

랜덤포레스트는 각 독립변수의 information gain을 평균내는 방식으로 변수 중요도(feature importance)를 계산할 수 있다. 하지만 각 트리의 분기에 무작위성이 개입하기 때문에 이 결과를 완전히 신뢰해서는 안된다. 참고용으로만 보자.

## 2. Boosting

Notations|Description
---|---
$C_m$|$m$개의 모델을 포함하는 모델 집합(commitee)
$k_m$|$m$번째로 포함된 모델

Aggregation이 복수의 모델을 훈련시켜 그 결과를 취합하는 병렬적인 방식이었다면, boosting은 순차적인 방식의 앙상블 기법이다. 부스팅은 기본적으로 이진 분류에서만 사용 가능하며, 하나의 모델에서 시작하여 앙상블 모델에 포함될 개별 모델을 하나씩 추가한다. 부스팅 기법은 $m-1$ 단계의 모델 집합 $C_{k-1}$의 성능을 가장 잘 보완하는 모델 $k$를 $m$번째 모델 $k_m$으로 선택한다. 앙상블 모델 $C_m$은 개별 모형의 결과와 가중치 $\alpha$를 선형조합한 값을 판별 함수로 사용한다. 여기에 판별 함수의 값이 양수이면 1, 음수이면 -1을 출력하는 $sign()$ 함수를 덧씌우면 최종 결과가 된다.

$$C_m(x_i)=sign({\alpha}_1 k_1(x_i) + \cdots +{\alpha}_m k_m(x_i))$$

### 2.1. Ada Boost(adaptive boost)

에이다부스트는 $i$번째 학습 데이터 $x_i$에 가중치 $w_i$를 부여한 후, 모형이 틀리게 예측한 데이터의 가중치를 모두 더한 값을 손실함수로 사용하고, 이 함수를 최소화하는 모형 $k$를 선택한다. 즉 $m$번째 모형 $k_m$에 대한 에이다부스트의 손실함수는 다음과 같이 정의된다.

$$Loss(C_m) = \sum^N_{i=1}w_{m,i}I(k_m(x_i){\neq}y_i)$$

$$
where \ w_{m,i} = \begin{cases}
w_{m,i} \times e^{-1} \ if \ C_{m-1} = y_i\\
w_{m,i} \times e^{1} \ if \ C_{m-1} \neq y_i\\
\end{cases}
$$
