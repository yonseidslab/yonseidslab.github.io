---
layout: post
title: "Bias-Variance Tradeoff?"
author: "JeonghyunGan"
categories: [DataScienceLab, Blog]
use_math: true
---

학회 면접에서 'Bias와 Variance를 Overfitting과 관련지어 설명하라'는 질문을 받았다. 당시에는 제대로 답변하지 못했다. 이후에 학회에서 이와 관련된 내용을 다루기도 했고, 계속해서 접하게 될 이슈이기 때문에 제대로 정리하고 넘어가는게 좋을 것 같아서 글을 남겨놓는다.

## Bias는 뭐고, Variance는 뭔가?

![Noise](/assets/article_images/Noise.png)

<br>
사실 Bias와 Variance 모두 통계학 수업에서 들어본 적은 있는 개념이다. 그런데 이 개념들이 왜 중요하고 오버피팅과는 또 무슨 상관인걸까? 우선 $y = f(x) + \epsilon$ 이라는 간단한 함수를 생각해보자. $y$는 우리가 예측하고자 하는 목적 변수이고, $x$에 대한 함수인 $f(x)$로 표현된다. 하지만 목적 변수가 하나의 함수에 의해 오차 없이 완벽하게 설명되기는 힘들다. 즉 $f(x)$라는 시그널에는 노이즈가 존재하기 마련이며, $\epsilon$은 이를 나타낸다. 실제 시그널인 $f(x)$를 알 수 있다면 좋겠지만, 대부분의 경우 데이터의 시그널은 알려져 있지 않다. 따라서 우리는 시그널에 가장 가까운 함수를 추정하고, 이를 $\hat{f}(x)$로 표기한다. 추정된 함수가 시그널에 가깝다면, 함수값은 실제 데이터와도 유사할 것이다. 따라서 추정은 $\sum(y-\hat{f}(x))^2$, 즉 오차제곱합을 최소화하는 $\hat{f}(x)$를 찾는 방식으로 이루어진다. 이제 이러한 관계를 통해서 Bias와 Variance를 설명해보자.

### Variance

> $Var[\hat{f}(x)] = E[ (\hat{f}(x) - E[\hat{f}(x)])^2] = E[\hat{f}(x)^2] - E[\hat{f}(x)]^2$

<br>
먼저 익숙한 Variance의 개념부터 다시 짚어보자. 분산은 편차 제곱의 평균이며, **예측치들이 평균으로부터 얼마나 산포되어 있는지를 나타내는 지표이다.** 분산이 크다면 예측치가 중심으로부터 넓게 퍼져있다는 말이 되고, 분산이 작다면 예측치가 중심 주변에 밀도있게 모여있다는 말이 된다.

<br>
### Bias

> $Bias[\hat{f}(x)] = E(\hat{f}(x)) - f(x)$

<br>
반면 Bias는 함수에 의한 예측값과 실제값의 사이의 관계다. 예측치의 평균과 실제 데이터의 차를 평균낸 것이 편향이다. 즉 **예측값이 실제값으로부터 얼마나 떨어져 있는지** 는 나타내는 지표이다. 편향이 크다면 함수가 실제 시그널을 정확하게 포착하지 못했다는 말이 되고, 편향이 작다면 함수가 실제 시그널을 꽤 정확하게 포착하고 있다는 말이 된다.


### Error의 관점에서 본 Bias와 Variance

> $E[(y - \hat{f}(x))^2]=  {Bias}^2 + Var + {\sigma}^2$

<br>
오차는 결국 Bias의 제곱과 Variance, $\{sigma}^2$(noise)의 합으로 표현된다(공식의 유도 과정은 [위키피디아](https://ko.wikipedia.org/wiki/%ED%8E%B8%ED%96%A5-%EB%B6%84%EC%82%B0_%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%93%9C%EC%98%A4%ED%94%84)를 참고하자). 이 때 $\{sigma}^2$은 모델과는 독립이기 때문에 학습을 통해서 줄이는 것이 불가능하다. 따라서 총 오차가 일정하다면, Bias와 Variance 사이에는 일정한 trade-off가 성립하게 되는 것이다. Bias를 줄이려면 Variance가 커질 수 밖에 없고, Variance를 줄이려면 Bias가 커질 수 밖에 없다. 따라서 Bias와 Variance를 적절한 수준으로 유지하면서 총 오차를 줄이는 것이 모델 최적화의 과정이라고 할 수 있다.

<br>
### 그래프로 보는 Bias-Variance

![Noise](/assets/article_images/Fitting.png)

<br>
위 그림은 실제 시그널인 sin함수로부터 나온 데이터와 이를 다항식으로 적합한 결과를 나타낸다. 일차식으로 적합했을 때, 분산은 작고 편향은 크다. 즉 일차식은 거의 모든 데이터에 대해서 일관된 결과를 내지만, 그 결과가 실제값으로부터 멀리 떨어져있다. 다항식의 차수가 높아질수록, 즉 함수(모델)이 복잡해질수록 분산을 커지고 편향은 줄어든다. 5차함수를 통해 적합했을 때는 실제 시그널과 꽤 유사한 그래프가 그려지는 것을 볼 수 있다. 모델이 복잡해질수록 편향이 감소하고 분산이 줄어드는 것이 일반적인 경향이다. 이 그래프만 봤을 때는 '무조건 편향이 작은 복잡한 모델이 더 좋은거 아니야?'라고 생각할 수 있다. 그러나 꼭 그렇지만은 않다.

![Overfitting](/assets/article_images/Overfitting.png)

<br>
분산이 크다는 것은 모델이 각기 다른 관측치들에 대해 내놓는 결과들의 차이가 크다는 뜻이며, 독립변수의 변동에 민감하게 반응한다는 뜻이다. 극단적으로 데이터의 모든 점들을 이어서 함수를 만드는 경우, 적어도 훈련 데이터에 대해서 오차는 0이 된다. 하지만 이러한 모델이 새로운 데이터에 대해서 적절한 예측을 내놓을 수 있을까? 그럴듯한 예측을 내놓을 수도 있겠지만, 오히려 단순한 모델보다 성능이 떨어질 가능성이 높다. 입력에 대해서 지나치게 민감하게 반응하여 일반화 능력이 떨어지기 때문이다.

## Overfitting, Bias-Variance Tradeoff

이제 편향과 분산의 개념, 그리고 오버피팅과의 관계에 대해서 대충은 감을 잡았을지 모르겠다. 일반적으로 오버피팅된 모델은 훈련 데이터에 대해서 작은 편향을 갖지만, 모델이 갖는 분산이 매우 크다. 즉 새로운 데이터에 대해서 내놓는 결과의 변동이 매우 심하다. Bias가 작다고 무조건 좋은 것도 아니고, Variance가 작다고 무조건 좋은 것도 아니다. 최적화는 오차를 최소로 만드는 Bias와 Variance의 균형을 찾아내는 일이다.
